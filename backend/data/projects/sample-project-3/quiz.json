{
  "questions": [
    {
      "id": "q1",
      "question": "在自然语言处理中，词向量(Word Embedding)的主要作用是什么？",
      "options": [
        "压缩文本",
        "将词语映射到向量空间",
        "删除停用词",
        "分词"
      ],
      "correctAnswer": 1,
      "explanation": "词向量将词语映射到高维向量空间，使语义相似的词语在向量空间中距离更近。"
    },
    {
      "id": "q2",
      "question": "BERT模型的核心创新是什么？",
      "options": [
        "使用CNN",
        "双向编码器",
        "循环结构",
        "注意力机制"
      ],
      "correctAnswer": 1,
      "explanation": "BERT使用双向编码器，能够同时利用上下文的前向和后向信息。"
    },
    {
      "id": "q3",
      "question": "在文本预处理中，词干提取(Stemming)的目的是什么？",
      "options": [
        "删除标点符号",
        "将词语还原为词根",
        "分词",
        "去除停用词"
      ],
      "correctAnswer": 1,
      "explanation": "词干提取将词语还原为词根形式，减少词汇表大小，提高模型泛化能力。"
    },
    {
      "id": "q4",
      "question": "Transformer架构中的自注意力机制解决了什么问题？",
      "options": [
        "计算复杂度",
        "序列长度限制",
        "梯度消失",
        "并行化训练"
      ],
      "correctAnswer": 3,
      "explanation": "自注意力机制使模型能够并行处理序列中的所有位置，大大提高了训练效率。"
    },
    {
      "id": "q5",
      "question": "在情感分析任务中，如何处理否定词的影响？",
      "options": [
        "忽略否定词",
        "使用n-gram特征",
        "上下文建模",
        "删除否定词"
      ],
      "correctAnswer": 2,
      "explanation": "通过上下文建模，模型能够理解否定词对情感的影响，如'不'、'没'等词。"
    }
  ]
}